{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldJ-xw-dVZNC"
      },
      "outputs": [],
      "source": [
        "# gbm_discrete_survival.py\n",
        "# Requirements: pandas, numpy, scikit-learn, lightgbm\n",
        "# pip install pandas numpy scikit-learn lightgbm\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from math import ceil\n",
        "from sklearn.model_selection import train_test_split\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import log_loss\n",
        "# Optional: concordance index from lifelines (for ranking)\n",
        "from lifelines.utils import concordance_index\n",
        "\n",
        "# ------------------------------\n",
        "# 1) Expand function (discrete intervals)\n",
        "# ------------------------------\n",
        "def expand_to_intervals(df, interval_days=7, max_days=None, cap_intervals=None):\n",
        "    \"\"\"\n",
        "    Expand listing-level dataset to discrete intervals for pooled-logistic (GBM) survival.\n",
        "    Input df must contain: listing_id, listing_date, days_to_lease, event (0/1), plus static features.\n",
        "    Returns expanded_df with one row per (listing_id, interval_index).\n",
        "    \"\"\"\n",
        "    required = {\"listing_id\", \"listing_date\", \"days_to_lease\", \"event\"}\n",
        "    if not required.issubset(set(df.columns)):\n",
        "        raise ValueError(f\"Missing required columns: {required - set(df.columns)}\")\n",
        "    if max_days is None:\n",
        "        max_days = int(df[\"days_to_lease\"].max())\n",
        "    max_intervals = ceil(max_days / interval_days) if max_days>0 else 1\n",
        "    if cap_intervals is not None:\n",
        "        max_intervals = min(max_intervals, cap_intervals)\n",
        "    rows = []\n",
        "    feature_cols = [c for c in df.columns if c not in [\"listing_date\", \"days_to_lease\", \"event\"]]\n",
        "    for _, r in df.iterrows():\n",
        "        n_intervals = int(ceil(max(1, r[\"days_to_lease\"]) / interval_days))\n",
        "        n_intervals = min(n_intervals, max_intervals)\n",
        "        for t in range(1, n_intervals+1):\n",
        "            start_day = (t-1)*interval_days + 1\n",
        "            end_day = t*interval_days\n",
        "            # event occurs in final interval only for events\n",
        "            event_interval = 1 if (int(r[\"event\"]) == 1 and t == n_intervals) else 0\n",
        "            rec = {\n",
        "                \"listing_id\": r[\"listing_id\"],\n",
        "                \"listing_date\": r[\"listing_date\"],\n",
        "                \"interval_index\": t,\n",
        "                \"interval_start_day\": start_day,\n",
        "                \"interval_end_day\": end_day,\n",
        "                \"time_elapsed_days\": min(end_day, int(r[\"days_to_lease\"]))\n",
        "            }\n",
        "            for c in feature_cols:\n",
        "                rec[c] = r[c]\n",
        "            rec[\"event_interval\"] = event_interval\n",
        "            rows.append(rec)\n",
        "    expanded = pd.DataFrame(rows)\n",
        "    # reorder\n",
        "    front = [\"listing_id\",\"listing_date\",\"interval_index\",\"interval_start_day\",\"interval_end_day\",\"time_elapsed_days\",\"event_interval\"]\n",
        "    others = [c for c in expanded.columns if c not in front]\n",
        "    return expanded[front+others]\n",
        "\n",
        "# ------------------------------\n",
        "# 2) Example: prepare train/val split (time-based)\n",
        "# ------------------------------\n",
        "# Assume `df` exists: listing-level dataframe for 2018-2024 with matured labels\n",
        "# Example: df = pd.read_parquet(\"listing_ttl.parquet\")\n",
        "\n",
        "# 1) Choose a cutoff date for train/validation split (by listing_date)\n",
        "#    e.g., train on listings before '2023-07-01', val on listings in [2023-07-01, 2024-06-30]\n",
        "train_cutoff = pd.to_datetime(\"2023-07-01\")\n",
        "\n",
        "train_listings = df[df[\"listing_date\"] < train_cutoff].copy()\n",
        "val_listings = df[df[\"listing_date\"] >= train_cutoff].copy()\n",
        "\n",
        "# 2) Optionally cap max_days to limit extreme tail expansion (keeps training small)\n",
        "interval_days = 7\n",
        "max_days = 180  # expand up to 180 days (tunable)\n",
        "cap_intervals = ceil(max_days / interval_days)\n",
        "\n",
        "# 3) Expand\n",
        "train_expanded = expand_to_intervals(train_listings, interval_days=interval_days, max_days=max_days, cap_intervals=cap_intervals)\n",
        "val_expanded = expand_to_intervals(val_listings, interval_days=interval_days, max_days=max_days, cap_intervals=cap_intervals)\n",
        "\n",
        "# ------------------------------\n",
        "# 3) Features & label for GBM\n",
        "# ------------------------------\n",
        "# Choose features (static as-of-listing_date) + time feature(s)\n",
        "static_features = [\"relative_price\", \"views_7d\", \"inquiries_7d\", \"maint_90\", \"bedrooms\", \"sqft\"]\n",
        "# Add the interval index/time elapsed as features to allow time-varying baseline hazard\n",
        "time_features = [\"interval_index\", \"time_elapsed_days\"]\n",
        "feature_cols = static_features + time_features\n",
        "\n",
        "# Optionally create interactions or engineered time features:\n",
        "# e.g., interval_index and interval_index^2, log-time, etc.\n",
        "train_expanded[\"interval_index_sq\"] = train_expanded[\"interval_index\"]**2\n",
        "val_expanded[\"interval_index_sq\"] = val_expanded[\"interval_index\"]**2\n",
        "feature_cols += [\"interval_index_sq\"]\n",
        "\n",
        "# ------------------------------\n",
        "# 4) Handle imbalance / weighting\n",
        "# ------------------------------\n",
        "# There are many more negative rows (no event) than positives.\n",
        "# One approach: weight positive examples higher, or use is_unbalance in LGBM.\n",
        "# Another: sample negatives for speed but keep positives.\n",
        "pos_weight = (train_expanded[\"event_interval\"]==0).sum() / max(1, (train_expanded[\"event_interval\"]==1).sum())\n",
        "# You can pass 'is_unbalance' or 'scale_pos_weight' to LightGBM; we'll use is_unbalance param.\n",
        "# Or create a weight column:\n",
        "train_expanded[\"sample_weight\"] = 1.0\n",
        "val_expanded[\"sample_weight\"] = 1.0\n",
        "# Increase weight for positives (optional)\n",
        "train_expanded.loc[train_expanded[\"event_interval\"]==1, \"sample_weight\"] = pos_weight\n",
        "\n",
        "# ------------------------------\n",
        "# 5) Train LightGBM per-interval classifier\n",
        "# ------------------------------\n",
        "lgb_params = {\n",
        "    \"objective\": \"binary\",\n",
        "    \"metric\": \"binary_logloss\",\n",
        "    \"learning_rate\": 0.05,\n",
        "    \"num_leaves\": 64,\n",
        "    \"min_data_in_leaf\": 50,\n",
        "    \"feature_fraction\": 0.9,\n",
        "    \"bagging_fraction\": 0.8,\n",
        "    \"bagging_freq\": 5,\n",
        "    \"verbosity\": -1,\n",
        "    \"seed\": 42,\n",
        "    # \"is_unbalance\": True,  # alternative to manual weights\n",
        "}\n",
        "\n",
        "dtrain = lgb.Dataset(train_expanded[feature_cols], label=train_expanded[\"event_interval\"], weight=train_expanded[\"sample_weight\"])\n",
        "dval = lgb.Dataset(val_expanded[feature_cols], label=val_expanded[\"event_interval\"], reference=dtrain)\n",
        "\n",
        "bst = lgb.train(lgb_params, dtrain, num_boost_round=1000, valid_sets=[dtrain, dval], early_stopping_rounds=50)\n",
        "\n",
        "# Save model\n",
        "bst.save_model(\"gbm_discrete_survival.txt\")\n",
        "\n",
        "# ------------------------------\n",
        "# 6) Predict hazards and compute survival for each listing\n",
        "# ------------------------------\n",
        "# Predict per-row hazard (probability of event in that interval)\n",
        "val_expanded[\"pred_hazard\"] = bst.predict(val_expanded[feature_cols], num_iteration=bst.best_iteration)\n",
        "\n",
        "# Compute survival by listing: S(t) = prod_{k<=t} (1 - h_k)\n",
        "val_expanded = val_expanded.sort_values([\"listing_id\", \"interval_index\"])\n",
        "val_expanded[\"one_minus_h\"] = 1.0 - val_expanded[\"pred_hazard\"]\n",
        "val_expanded[\"survival\"] = val_expanded.groupby(\"listing_id\")[\"one_minus_h\"].cumprod()\n",
        "\n",
        "# Probability leased within t intervals = 1 - survival(t)\n",
        "val_expanded[\"p_leased_within_interval\"] = 1.0 - val_expanded[\"survival\"]\n",
        "\n",
        "# Example: extract P(leased within 30 days) depending on interval length\n",
        "horizon_days = 30\n",
        "horizon_interval = ceil(horizon_days / interval_days)\n",
        "p30 = val_expanded[val_expanded[\"interval_index\"] == horizon_interval][[\"listing_id\",\"p_leased_within_interval\"]].set_index(\"listing_id\")\n",
        "# p30 is the per-listing probability leased within ~30 days\n",
        "\n",
        "# ------------------------------\n",
        "# 7) Evaluation ideas\n",
        "# ------------------------------\n",
        "# - Evaluate per-interval logloss on validation set:\n",
        "val_logloss = log_loss(val_expanded[\"event_interval\"], val_expanded[\"pred_hazard\"])\n",
        "print(\"Validation per-interval logloss:\", val_logloss)\n",
        "\n",
        "# - For ranking (who leases sooner), compute concordance index using predicted risk (1 - survival at some time)\n",
        "#   Here we compute predicted median risk up to 90 days as a single score for ordering:\n",
        "pred_score = val_expanded[val_expanded[\"interval_index\"] <= ceil(90/interval_days)].groupby(\"listing_id\")[\"p_leased_within_interval\"].max()\n",
        "# Build arrays for concordance: ground truth is days_to_lease and event from original validation listings\n",
        "val_meta = val_listings.set_index(\"listing_id\")\n",
        "# Align\n",
        "common = pred_score.index.intersection(val_meta.index)\n",
        "c_index = concordance_index(val_meta.loc[common,\"days_to_lease\"], -pred_score.loc[common].values, val_meta.loc[common,\"event\"])\n",
        "print(\"Concordance index (higher better):\", c_index)\n",
        "\n",
        "# - Brier score / calibration: compute predicted survival vs observed at time horizons (requires grouping by listing).\n",
        "# See scikit-survival or custom implementations.\n",
        "\n",
        "# ------------------------------\n",
        "# 8) Notes & production tips\n",
        "# ------------------------------\n",
        "# - Use interval length thoughtfully: 7 days (weekly) is common; shorter intervals => bigger dataset.\n",
        "# - Use time features to let model learn baseline hazard (interval_index, time_elapsed_days, seasonality flags).\n",
        "# - For time-varying covariates (e.g., views_7d), you can recompute feature as-of each interval and include it per interval row.\n",
        "# - Keep splitting by listing_date to avoid leakage.\n",
        "# - Use LightGBM's \"is_unbalance\" or sample weights for class imbalance.\n",
        "# - When scoring current active listings, expand them into intervals out to desired horizon (e.g., 90 days),\n",
        "#   predict hazards, and compute survival curves.\n"
      ]
    }
  ]
}