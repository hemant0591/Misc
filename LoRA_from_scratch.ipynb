{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ve1iEh9FTnTN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA implementation for the linear layer of the model\n",
        "\n",
        "class LoRALinear(nn.Module):\n",
        "  def __init__(self, orig_params, r=0, alpha=None, dropout=0.1, merge=False, init_scale=0.1):\n",
        "    super().__init__()\n",
        "    self.orig = orig_params\n",
        "    self.r = r\n",
        "    self.merge = merge\n",
        "    self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
        "    d_in = orig_params.in_features\n",
        "    d_out = orig_params.out_features\n",
        "\n",
        "    if alpha is None:\n",
        "      alpha = r\n",
        "    self.scaling = alpha / r\n",
        "\n",
        "    # LoRA params: A (r x d_in), B (d_out x r)\n",
        "    self.A = nn.Parameter(torch.randn(r, d_in) * init_scale)\n",
        "    self.B = nn.Parameter(torch.zeros(d_out, r))\n",
        "\n",
        "    # Freeze original weights\n",
        "    for p in self.orig.Parameter():\n",
        "      p.requires_grad = False\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x: [batch, ..., d_in]\n",
        "    # orig_out: regular linear\n",
        "    orig_out = self.orig(x)\n",
        "\n",
        "    if self.merge:\n",
        "      # If merged, orig already contains LoRA contribution\n",
        "      return orig_out\n",
        "\n",
        "    # LoRA path: compute A x -> (r)\n",
        "    lora_inter = self.dropout(x) @ self.A.t() # shape: [..., r]\n",
        "    lora_out = lora_inter @ self.B.t() # shape: [..., d_out]\n",
        "    lora_out = self.scaling * lora_out\n",
        "\n",
        "    return orig_out + lora_out\n",
        "\n",
        "  def merge_weights(self):\n",
        "    \"\"\"Fuse LoRA into orig weight: W = W + scaling * (B @ A)\"\"\"\n",
        "    if hasattr(self.orig, 'weight') and not self.merge:\n",
        "      with torch.no_grad():\n",
        "        delta = (self.B @ self.A) * self.scaling\n",
        "        self.orig.weight.data += delta\n",
        "      self.merge = True\n",
        "    # del self.A, self.B\n",
        "\n"
      ],
      "metadata": {
        "id": "GzLbpJD9Tv70"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hyRkBWmrXQfK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}